{"filename":"hadoop-2-7-2-single-node-and-cluster-mode-installation-guide","md":"# Hadoop 2.7.2 单节点与集群安装部署\n\n```metadata\ntitle: Hadoop 2.7.2 单节点与集群安装部署\ncreated: 2016-04-09\nupdated: 2016-04-09\ncategory: Note\ntags:\n- Hadoop\n- Linux\ncover: https://picsum.photos/id/278/800/300\n```\n\n## Background\n\n最近需要做一些大数据相关项目,至少需要搭建 Hadoop 的基本环境. 由于用到的是目前版本号最高的 Hadoop 2.7.2.跟文档比较多的 2.6 以下的版本相比,在部署集群的时候遇见了很多坑. 所以写一份安装指南,记录一下跌坑的过程,以示警惕.\n\n在环境搭建的过程中,参考了以下两篇文章: 其中 Apache 的官方 Wiki 文档写的真难读. 建议直接先看一遍 aws 的指南再动手.\n\n[https://wiki.apache.org/hadoop/GettingStartedWithHadoop](https://wiki.apache.org/hadoop/GettingStartedWithHadoop) [https://rstudio-pubs-static.s3.amazonaws.com/](https://rstudio-pubs-static.s3.amazonaws.com/78508_abe89197267240dfb6f4facb361a20ed.html)\n\n## Table Of Content\n\n暂不讨论 Hadoop 及基于其应用的场景描述,文本只讨论基本的环境搭建步骤和与之涉及的知识点. 按照顺序总结出本文的内容节点.\n\n1. 宏观了解在集群上部署 Hadoop 的过程\n2. 虚拟机基本网络配置与机器配置\n3. 下载并解压 Hadoop\n4. 创建专为运行 Hadoop 的用户\n5. 环境变量的设定\n6. 修改 Hadoop 配置文件\n7. 启动 Hadoop 服务\n\n8. 防跌坑指南\n\n## Environment Setup\n\n### Overview: How Developer deploy Hadoop in cluster mode\n\n通常来说,一个运维工程师是如何部署一个 Hadoop 集群呢? 集群可以当成 1 台 Master 机器和多台 Slaves 机器. 在全新的 Linux 机器群中创建 Hadoop 集群,按我的理解可以分成以下几步.\n\n1. 在 Master 上下载 Hadoop,并修改对应的 Hadoop 配置文件.\n2. 将修改好配置的 Hadoop 目录打包,分发到各个 Slave 中,解压到固定的执行目录.\n3. 修改所有机器的 hosts 文件,将局域网中的所有 ip-hostname 进行 mapping.\n4. 在所有机器上安装 ssh,Master 和 Slaves 之间将通过 ssh 进行运行时的通讯控制.\n5. 在 Master 上启动 Hadoop 服务.统一管理所有 Slave 节点.\n\n### Network and Information about Virtual Machine\n\n因为只是实验集群的部署,所以没有用到真机. 实际上虚拟机内部的多台机器所组成的集群,其实总的 I/O 还是会被物理机器限制.\n\n我将使用的是 1 台 Master 和两台 Slaves 三台机器都是`vmware`上的虚拟机,网络方式都是以 NAT 桥接具体的配置如下:\n\n- Master: ip:192.168.239.142 hostname:elementary-os cpu:4 core ram:16G\n\n- Slaves: ip:192.168.239.144,192.168.239.145 hostname:hd-worker-a,hd-worker-b cpu:1 core ram:4G harddisk:20G\n\nP.S. Master 和 Slaves 都是基于 64 位的`Ubuntu14.04.3LTS`内核的 Linux. 可以视为都是通过直接安装`Ubuntu14.04.3.LTS`.\n\n为了通过`/etc/hosts`文件通过机器名访问对应的 ip, 在每一个节点上面都修改对应的`/etc/hosts`文件,将集群中所有节点加到文件里面\n\n```bash\n$ sudo vi /etc/hosts\n```\n\n在文件底部添加以下几行(实际操作中,请将 hostname 替换成自己实际机器的 hostname 与 ip)\n\n```\n# Hadoop Cluster Setup\n## Master\n192.168.239.142   elementary-os\n\n## Slaves\n192.168.239.144   hd-worker-a\n192.168.239.145   hd-worker-b\n```\n\n### Download Hadoop\n\n下载地址:[https://mirrors.noc.im/apache/hadoop/common/current/](https://mirrors.noc.im/apache/hadoop/common/current/)\n\n下载之后,会得到一个`Hadoop2.7.2`的解压包. 在下一步章节我们将会将其移动到其他目录.\n\n### Add Hadoop Group and User\n\n在所有节点上都创建一个名为`hduser`的 user,并将其加到 sudo 列表里面. 在接下来的所有 bash 命令,默认都通过该创建的`hduser`来执行.\n\n```\n$ sudo addgroup hadoop\n$ sudo adduser --ingroup hadoop hduser\n$ sudo adduser hduser sudo\n```\n\n### Installing SSH and Copy Public Key to remote machine\n\n什么是 SSH SSH (“Secure SHell”) is a protocol for securely accessing one machine from another. Hadoop uses SSH for accessing another slaves nodes to start and manage all HDFS and MapReduce daemons. Hadoop 通过 ssh 之间来通讯和管理节点之间的通讯.\n\n```\n$ sudo apt-get install openssh-server\n```\n\nTIPS: 通常来说,ssh 远程到另一台安装了 ssh 的机器上,通过`ssh {username}@{hostname}`,之后输入密码便可以进入. 对于一些自动化部署的脚本来说自动输入密码,还需要在脚本里面写下密码. 怎么可能如此的不科学?\n\n所以需要通过 ssh key 公钥来进行认证,达到无密码传输的过程. 假设我们需要在机器上 A 通过 ssh 远程到机器 B 且不需要密码,步骤如下:\n\n1. 在机器 A 上生成自己的 ssh 公钥与密钥\n\n```bash\n$ ssh-keygen -t rsa\n```\n\n此举将会在 user 目录下的`~/.ssh`文件夹创建对应的 `id_rsa`和`id_rsa.pub`文件. 其中`id_rsa.pub`就是公钥文件\n\n2. 在机器 A 上将自己的公钥复制到远程主机上\n\n```bash\n$ ssh-copy-id {username}@B\n$ {username}@B password:\n$ #此时输入用户密码\n```\n\n此举会在远程主机 B 的对应 user 的 home/.ssh 目录下创建`authorized_key`文件. 该公钥已经信任,拥有这个公钥的 A 主机用户可以直接通过`ssh {username}@B`不输入密码而直接远程到 B\n\nOK.在了解到这一步之后,大概知道一台机器的主机需要如何配置 ssh 了. 因为在 Hadoop 集群中,Master 与每一台 Slaves 都需要进行 ssh 通讯, 所以需要在 Hadoop 中每一台机器都生成自己的 ssh 公钥,然后与 Master 互相进行公钥传输动作.\n\n在我自己的集群中,进行了 4 次`ssh-copy-id`操作:\n\n1. elementary-os -> hd-worker-a\n2. hd-worker-a -> elementary-os\n3. elementary-os -> hd-worker-b\n4. hd-worker-b -> elementary-os\n\n### Basic Environment Setup\n\n在修改 Hadoop 的配置之前,需要进行配置的是所有节点的环境变量设置与必要的基础程序.\n\n#### JDK\n\nHadoop 运行在 Java 环境中,所以每个节点都需要安装 JDK. 需要保证的是确保每一台节点上安装的 JDK 版本一致. P.S 我自己是 Master OpenJDK-8 + Slaves OpenJDK-7. 目前还是正常运行的 (顺便吐槽一下 `Ubuntu14.04`默认的 apt-get 源,相当傻逼.在不添加自己订阅的其他源的情况下连 OpenJDK8 的地址都没有,而且如果安装 Git 之类的工具,为求稳定居然用的是 1.7 以下的版本. 这也是为什么我日常开发用的是`elementary-os`,虽然也是基于 ubuntu14 的内核, 但是 elementary-os 修改了其默认的 apt 源,ui 看起来也更加顺眼)\n\n```bash\n$ sudo apt-get install openjdk-7-jdk\n```\n\n通过此举,安装的默认的 jdk 路径是`/usr/lib/jvm/java-7-openjdk-amd64`. OpenJDK8 同理. OracleJDK 也推荐复制到`/usr/lib/jvm`目录下.(守序善良 Linux 派优雅的约定之一)\n\n记住这里咯.在下面我们会将这个 JDK 的目录,加到当前用户`hduser`的`.bashrc`中.\n\n### Configure Hadoop\n\n终于到了这一步. 建议首先在 Master 上机器修改好 Hadoop 的配置.然后压缩该文件夹,复制到其他 Slave 节点上的同一目录.\n\n#### Unpack and move hadoop folder\n\n假设下载好的 hadoop-2.7.2.tar.gz 在 当前用户的`Downloads`文件夹中. 解压完毕之后,将其移动到`/usr/local`下,并更名为`hadoop`\n\n```\n$ mv hadoop-2.7.2 /usr/local/hadoop\n```\n\n#### Update Environment File\n\n在配置 Hadoop 的过程中,下列配置文件将会被修改.\n\n> ~/.bashrc /usr/local/hadoop/etc/hadoop/slaves /usr/local/hadoop/etc/hadoop/hadoop-env.sh /usr/local/hadoop/etc/hadoop/core-site.xml /usr/local/hadoop/etc/hadoop/yarn-site.xml /usr/local/hadoop/etc/hadoop/mapred-site.xml /usr/local/hadoop/etc/hadoop/hdfs-site.xml\n\n##### ~/.bashrc\n\n还记得之前提过的 JDK 路径吗,将其配置成`JAVA_HOME` 修改当前用户的 bash 配置文件,将其加到.bashrc 的底部\n\n```bash\n$ cd ~\n$ vi .bashrc\n```\n\n```sh\n#Hadoop variables\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/\nexport HADOOP_INSTALL=/usr/local/hadoop\nexport PATH=$PATH:$HADOOP_INSTALL/bin\nexport PATH=$PATH:$HADOOP_INSTALL/sbin\nexport HADOOP_MAPRED_HOME=$HADOOP_INSTALL\nexport HADOOP_COMMON_HOME=$HADOOP_INSTALL\nexport HADOOP_HDFS_HOME=$HADOOP_INSTALL\nexport YARN_HOME=$HADOOP_INSTALL\n```\n\n##### /usr/local/hadoop/etc/hadoop/hadoop-env.sh\n\n还是跟上面一样,需要将 JDK 的路径设置成`JAVA_HOME`\n\n```\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/\n```\n\n##### /usr/local/hadoop/etc/hadoop/core-site.xml\n\n在`<configuartion></configuration>`之间添加一个 fs.default.name,其值为 master 机器的 9000 端口. 譬如我的 master 机器是`elementary-os`,则 value 是`hdfs://elementary-os:9000` P.S.接下来的变量`{master-hostname}`请自行替换成自己的 master 的机器名.\n\n```xml\n<configuration>\n  <property>\n    <name>fs.default.name</name>\n    <value>hdfs://{master-hostname}:9000</value>\n  </property>\n  <property>\n    <name>hadoop.tmp.dir</name>\n    <value>file:/usr/local/hadoop_store/tmp</value>\n  </property>\n</configuration>\n```\n\n#### /usr/local/hadoop/etc/hadoop/yarn-site.xml\n\n在`<configuartion></configuration>`之间添加:\n\n```xml\n<configuration>\n  <property>\n    <name>yarn.nodemanager.aux-services</name>\n    <value>mapreduce_shuffle</value>\n  </property>\n  <property>\n    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n    <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.hostname</name>\n    <value>{master-hostname}</value>\n  </property>\n</configuration>\n\n```\n\n##### /usr/local/hadoop/etc/hadoop/mapred-site.xml\n\n`mapred-site.xml`默认是不存在的. 但是有一份模板文件`mapred-site.xml.template`,我们将其复制并重命名成`mapred-site.xml`\n\n```bash\n$ cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml\n```\n\n在`<configuartion></configuration>`之间添加:\n\n```xml\n<configuartion>\n  <property>\n    <name>mapreduce.framework.name</name>\n    <value>yarn</value>\n  </property>\n  <property>\n    <name>mapred.job.tracker</name>\n    <value>{master-hostname}:9001</value>\n  </property>\n  <property>\n    <name>mapreduce.jobhistory.address</name>\n    <value>{master-hostname}:10020</value>\n  </property>\n  <property>\n    <name>mapreduce.jobhistory.webapp.address</name>\n    <value>{master-hostname}:19888</value>\n  </property>\n</configuartion>\n```\n\n##### /usr/local/hadoop/etc/hadoop/hdfs-site.xml\n\n在修改`hdfs-site.xml`这个配置文件之前,我们需要知道更多的一件事. hdfs 的块状文件,储存在一个指定的目录中. 按照官方文档的推荐,和网上一些文件夹的路径的约定,我们将这个 hdfs 的文件储存目录叫做`hadoop_store`.绝对路径为`/usr/local/hadoop_store`\n\n于是 hadoop 的相关文件夹就变成了两个:\n\n> /usr/local/hadoop /usr/local/hadoop_store\n\n由于读写权限问题,我们需要将`hadoop_store`的权限改成任意可读可写\n\n```bash\n$ sudo mkdir -p /usr/local/hadoop_store\n$ sudo chmod -R 777 /usr/local/hadoop_store\n```\n\n然后再在配置文件里面加入\n\n```xml\n<configuartion>\n  <property>\n    <name>dfs.namenode.secondary.http-address</name>\n    <value>{master-hostname}:50090</value>\n  </property>\n  <property>\n    <name>dfs.replication</name>\n    <value>1</value>\n  </property>\n  <property>\n    <name>dfs.namenode.name.dir</name>\n    <value>file:/usr/local/hadoop_store/hdfs/namenode</value>\n  </property>\n  <property>\n    <name>dfs.datanode.data.dir</name>\n    <value>file:/usr/local/hadoop_store/hdfs/datanode</value>\n  </property>\n</configuartion>\n```\n\n##### slaves\n\n`slaves`文件里面存储的是作为 slave 的节点的机器名. 以行为单位,一行一个. 默认只有一行 localhost. 从一般的集群角度来说,Master 不应该担当 Worker 的角色(老湿布置作业给小学僧,自己是不会一起做作业的) 所以 slaves 文件一般只写 slave 节点的名字,即 slave 节点作为 datanode,master 节点仅仅作为 namenode.\n\n但是由于我是一名好老湿,所以在本机配置中 master 也充当了 worker 的角色,所以本机是这样改的:\n\n```\nelementary-os\nhd-worker-a\nhd-worker-b\n```\n\n致此,所有的配置文件已经修改完毕. 可以将 master 上的 hadoop 文件夹压缩并且分发到各个 slave 节点上.\n\n#### Last Configure : Format Namenode\n\n最后一步配置,初始格式化 hdfs\n\n```bash\n$ cd /usr/local/hadoop/\n$ hdfs namenode -format\n```\n\n### Start all Hadoop deamons\n\n启动 Hadoop 服务.\n\n```bash\n$ su hduser\n$ cd /usr/local/hadoop/\n$ sbin/start-dfs.sh\n$ sbin/start-yarn.sh\n```\n\n如果启动成功,在 master 节点上通过 jps 命令查看,应该包含如下 hadoop 进程\n\n```\nhduser@elementary-os:~$ jps\n51288 Jps\n22914 ResourceManager\n22361 NameNode\n23229 NodeManager\n22719 SecondaryNameNode\n```\n\n在 slave 节点上通过 jps 命令查看,应该包含如下 hadoop 进程\n\n```\nhduser@hd-worker-a:~$ jps\n6284 NodeManager\n6150 DateNode\n6409 Jps\n```\n\n或者可以通过浏览器访问[https://master:8088](https://master:8088) 或者[https://master:50070](https://master:50070) 查看 Hadoop 服务状态.\n\n![Nodes of the cluster](https://img.alicdn.com/tfscom/TB1vHd.MpXXXXXDXFXXXXXXXXXX.png) ![Namenode information](https://img.alicdn.com/tfscom/TB16f8YMpXXXXbJXVXXXXXXXXXX.png)\n\nP.S.关于`jps`命令. jps 位于 jdk 的 bin 目录下,其作用是显示当前系统的 java 进程情况,及其 id 号. jps 相当于 linux 进程工具 ps,但是不支持管道命令 grep jps 并不使用应用程序名来查找 JVM 实例.\n\n## Trouble Shooting\n\n防跌坑指南. 记录了在 Hadoop 环境搭建过程中所遇到的坑\n\n### Number of Live DataNode:0\n\n通过`start-dfs.sh`启动了 hadoop 多个节点的 datanode, 且通过`jps`命令能够看到正常的 datanode 和 resourcemanager 进程, 为什么 live datanode 数目为 0,或者只有 master 的那个 datanode?\n\n可通过以下方法排除:\n\n1. 关闭所有节点的防火墙(ubuntu): 先查看防火墙状态\n\n```bash\n$ sudo ufw status\n```\n\n如果不是 disabled,则禁用\n\n```bash\n$ sudo ufw disable\n```\n\n2. 在 hadoop 服务运行的时候,关闭 namenode 的安全模式\n\n```bash\n$ hadoop dfsadmin -safemode leave\n```\n\n3. 在关闭 hadoop 服务的情况下,删除所有的日志文件,存储文件并重新 format 确保`hadoop_store`文件夹下的所有文件夹权限都是 777\n\n```\n$sudo rm -r /usr/local/hadoop/logs\n$sudo rm -r /usr/local/hadoop_store/tmp\n$sudo rm -r /usr/local/hadoop_store/hdfs\n$sidp hdfs namenode -format\n```\n","html":"<h1 id=\"hadoop-272-单节点与集群安装部署\">Hadoop 2.7.2 单节点与集群安装部署</h1>\n<h2 id=\"background\">Background</h2>\n<p>最近需要做一些大数据相关项目,至少需要搭建 Hadoop 的基本环境. 由于用到的是目前版本号最高的 Hadoop 2.7.2.跟文档比较多的 2.6 以下的版本相比,在部署集群的时候遇见了很多坑. 所以写一份安装指南,记录一下跌坑的过程,以示警惕.</p>\n<p>在环境搭建的过程中,参考了以下两篇文章: 其中 Apache 的官方 Wiki 文档写的真难读. 建议直接先看一遍 aws 的指南再动手.</p>\n<p><a href=\"https://wiki.apache.org/hadoop/GettingStartedWithHadoop\">https://wiki.apache.org/hadoop/GettingStartedWithHadoop</a> <a href=\"https://rstudio-pubs-static.s3.amazonaws.com/78508_abe89197267240dfb6f4facb361a20ed.html\">https://rstudio-pubs-static.s3.amazonaws.com/</a></p>\n<h2 id=\"table-of-content\">Table Of Content</h2>\n<p>暂不讨论 Hadoop 及基于其应用的场景描述,文本只讨论基本的环境搭建步骤和与之涉及的知识点. 按照顺序总结出本文的内容节点.</p>\n<ol>\n<li>\n<p>宏观了解在集群上部署 Hadoop 的过程</p>\n</li>\n<li>\n<p>虚拟机基本网络配置与机器配置</p>\n</li>\n<li>\n<p>下载并解压 Hadoop</p>\n</li>\n<li>\n<p>创建专为运行 Hadoop 的用户</p>\n</li>\n<li>\n<p>环境变量的设定</p>\n</li>\n<li>\n<p>修改 Hadoop 配置文件</p>\n</li>\n<li>\n<p>启动 Hadoop 服务</p>\n</li>\n<li>\n<p>防跌坑指南</p>\n</li>\n</ol>\n<h2 id=\"environment-setup\">Environment Setup</h2>\n<h3 id=\"overview-how-developer-deploy-hadoop-in-cluster-mode\">Overview: How Developer deploy Hadoop in cluster mode</h3>\n<p>通常来说,一个运维工程师是如何部署一个 Hadoop 集群呢? 集群可以当成 1 台 Master 机器和多台 Slaves 机器. 在全新的 Linux 机器群中创建 Hadoop 集群,按我的理解可以分成以下几步.</p>\n<ol>\n<li>在 Master 上下载 Hadoop,并修改对应的 Hadoop 配置文件.</li>\n<li>将修改好配置的 Hadoop 目录打包,分发到各个 Slave 中,解压到固定的执行目录.</li>\n<li>修改所有机器的 hosts 文件,将局域网中的所有 ip-hostname 进行 mapping.</li>\n<li>在所有机器上安装 ssh,Master 和 Slaves 之间将通过 ssh 进行运行时的通讯控制.</li>\n<li>在 Master 上启动 Hadoop 服务.统一管理所有 Slave 节点.</li>\n</ol>\n<h3 id=\"network-and-information-about-virtual-machine\">Network and Information about Virtual Machine</h3>\n<p>因为只是实验集群的部署,所以没有用到真机. 实际上虚拟机内部的多台机器所组成的集群,其实总的 I/O 还是会被物理机器限制.</p>\n<p>我将使用的是 1 台 Master 和两台 Slaves 三台机器都是<code>vmware</code>上的虚拟机,网络方式都是以 NAT 桥接具体的配置如下:</p>\n<ul>\n<li>\n<p>Master: ip:192.168.239.142 hostname:elementary-os cpu:4 core ram:16G</p>\n</li>\n<li>\n<p>Slaves: ip:192.168.239.144,192.168.239.145 hostname:hd-worker-a,hd-worker-b cpu:1 core ram:4G harddisk:20G</p>\n</li>\n</ul>\n<p>P.S. Master 和 Slaves 都是基于 64 位的<code>Ubuntu14.04.3LTS</code>内核的 Linux. 可以视为都是通过直接安装<code>Ubuntu14.04.3.LTS</code>.</p>\n<p>为了通过<code>/etc/hosts</code>文件通过机器名访问对应的 ip, 在每一个节点上面都修改对应的<code>/etc/hosts</code>文件,将集群中所有节点加到文件里面</p>\n<pre><code class=\"hljs bash\">$ sudo vi /etc/hosts\n</code></pre>\n<p>在文件底部添加以下几行(实际操作中,请将 hostname 替换成自己实际机器的 hostname 与 ip)</p>\n<pre><code># Hadoop Cluster Setup\n## Master\n192.168.239.142   elementary-os\n\n## Slaves\n192.168.239.144   hd-worker-a\n192.168.239.145   hd-worker-b\n</code></pre>\n<h3 id=\"download-hadoop\">Download Hadoop</h3>\n<p>下载地址:<a href=\"https://mirrors.noc.im/apache/hadoop/common/current/\">https://mirrors.noc.im/apache/hadoop/common/current/</a></p>\n<p>下载之后,会得到一个<code>Hadoop2.7.2</code>的解压包. 在下一步章节我们将会将其移动到其他目录.</p>\n<h3 id=\"add-hadoop-group-and-user\">Add Hadoop Group and User</h3>\n<p>在所有节点上都创建一个名为<code>hduser</code>的 user,并将其加到 sudo 列表里面. 在接下来的所有 bash 命令,默认都通过该创建的<code>hduser</code>来执行.</p>\n<pre><code>$ sudo addgroup hadoop\n$ sudo adduser --ingroup hadoop hduser\n$ sudo adduser hduser sudo\n</code></pre>\n<h3 id=\"installing-ssh-and-copy-public-key-to-remote-machine\">Installing SSH and Copy Public Key to remote machine</h3>\n<p>什么是 SSH SSH (“Secure SHell”) is a protocol for securely accessing one machine from another. Hadoop uses SSH for accessing another slaves nodes to start and manage all HDFS and MapReduce daemons. Hadoop 通过 ssh 之间来通讯和管理节点之间的通讯.</p>\n<pre><code>$ sudo apt-get install openssh-server\n</code></pre>\n<p>TIPS: 通常来说,ssh 远程到另一台安装了 ssh 的机器上,通过<code>ssh {username}@{hostname}</code>,之后输入密码便可以进入. 对于一些自动化部署的脚本来说自动输入密码,还需要在脚本里面写下密码. 怎么可能如此的不科学?</p>\n<p>所以需要通过 ssh key 公钥来进行认证,达到无密码传输的过程. 假设我们需要在机器上 A 通过 ssh 远程到机器 B 且不需要密码,步骤如下:</p>\n<ol>\n<li>在机器 A 上生成自己的 ssh 公钥与密钥</li>\n</ol>\n<pre><code class=\"hljs bash\">$ ssh-keygen -t rsa\n</code></pre>\n<p>此举将会在 user 目录下的<code>~/.ssh</code>文件夹创建对应的 <code>id_rsa</code>和<code>id_rsa.pub</code>文件. 其中<code>id_rsa.pub</code>就是公钥文件</p>\n<ol start=\"2\">\n<li>在机器 A 上将自己的公钥复制到远程主机上</li>\n</ol>\n<pre><code class=\"hljs bash\">$ ssh-copy-id {username}@B\n$ {username}@B password:\n$ <span class=\"hljs-comment\">#此时输入用户密码</span>\n</code></pre>\n<p>此举会在远程主机 B 的对应 user 的 home/.ssh 目录下创建<code>authorized_key</code>文件. 该公钥已经信任,拥有这个公钥的 A 主机用户可以直接通过<code>ssh {username}@B</code>不输入密码而直接远程到 B</p>\n<p>OK.在了解到这一步之后,大概知道一台机器的主机需要如何配置 ssh 了. 因为在 Hadoop 集群中,Master 与每一台 Slaves 都需要进行 ssh 通讯, 所以需要在 Hadoop 中每一台机器都生成自己的 ssh 公钥,然后与 Master 互相进行公钥传输动作.</p>\n<p>在我自己的集群中,进行了 4 次<code>ssh-copy-id</code>操作:</p>\n<ol>\n<li>elementary-os -&gt; hd-worker-a</li>\n<li>hd-worker-a -&gt; elementary-os</li>\n<li>elementary-os -&gt; hd-worker-b</li>\n<li>hd-worker-b -&gt; elementary-os</li>\n</ol>\n<h3 id=\"basic-environment-setup\">Basic Environment Setup</h3>\n<p>在修改 Hadoop 的配置之前,需要进行配置的是所有节点的环境变量设置与必要的基础程序.</p>\n<h4 id=\"jdk\">JDK</h4>\n<p>Hadoop 运行在 Java 环境中,所以每个节点都需要安装 JDK. 需要保证的是确保每一台节点上安装的 JDK 版本一致. P.S 我自己是 Master OpenJDK-8 + Slaves OpenJDK-7. 目前还是正常运行的 (顺便吐槽一下 <code>Ubuntu14.04</code>默认的 apt-get 源,相当傻逼.在不添加自己订阅的其他源的情况下连 OpenJDK8 的地址都没有,而且如果安装 Git 之类的工具,为求稳定居然用的是 1.7 以下的版本. 这也是为什么我日常开发用的是<code>elementary-os</code>,虽然也是基于 ubuntu14 的内核, 但是 elementary-os 修改了其默认的 apt 源,ui 看起来也更加顺眼)</p>\n<pre><code class=\"hljs bash\">$ sudo apt-get install openjdk-7-jdk\n</code></pre>\n<p>通过此举,安装的默认的 jdk 路径是<code>/usr/lib/jvm/java-7-openjdk-amd64</code>. OpenJDK8 同理. OracleJDK 也推荐复制到<code>/usr/lib/jvm</code>目录下.(守序善良 Linux 派优雅的约定之一)</p>\n<p>记住这里咯.在下面我们会将这个 JDK 的目录,加到当前用户<code>hduser</code>的<code>.bashrc</code>中.</p>\n<h3 id=\"configure-hadoop\">Configure Hadoop</h3>\n<p>终于到了这一步. 建议首先在 Master 上机器修改好 Hadoop 的配置.然后压缩该文件夹,复制到其他 Slave 节点上的同一目录.</p>\n<h4 id=\"unpack-and-move-hadoop-folder\">Unpack and move hadoop folder</h4>\n<p>假设下载好的 hadoop-2.7.2.tar.gz 在 当前用户的<code>Downloads</code>文件夹中. 解压完毕之后,将其移动到<code>/usr/local</code>下,并更名为<code>hadoop</code></p>\n<pre><code>$ mv hadoop-2.7.2 /usr/local/hadoop\n</code></pre>\n<h4 id=\"update-environment-file\">Update Environment File</h4>\n<p>在配置 Hadoop 的过程中,下列配置文件将会被修改.</p>\n<blockquote>\n<p>~/.bashrc /usr/local/hadoop/etc/hadoop/slaves /usr/local/hadoop/etc/hadoop/hadoop-env.sh /usr/local/hadoop/etc/hadoop/core-site.xml /usr/local/hadoop/etc/hadoop/yarn-site.xml /usr/local/hadoop/etc/hadoop/mapred-site.xml /usr/local/hadoop/etc/hadoop/hdfs-site.xml</p>\n</blockquote>\n<h5 id=\"~bashrc\">~/.bashrc</h5>\n<p>还记得之前提过的 JDK 路径吗,将其配置成<code>JAVA_HOME</code> 修改当前用户的 bash 配置文件,将其加到.bashrc 的底部</p>\n<pre><code class=\"hljs bash\">$ <span class=\"hljs-built_in\">cd</span> ~\n$ vi .bashrc\n</code></pre>\n<pre><code class=\"hljs sh\"><span class=\"hljs-comment\">#Hadoop variables</span>\n<span class=\"hljs-built_in\">export</span> JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/\n<span class=\"hljs-built_in\">export</span> HADOOP_INSTALL=/usr/<span class=\"hljs-built_in\">local</span>/hadoop\n<span class=\"hljs-built_in\">export</span> PATH=<span class=\"hljs-variable\">$PATH</span>:<span class=\"hljs-variable\">$HADOOP_INSTALL</span>/bin\n<span class=\"hljs-built_in\">export</span> PATH=<span class=\"hljs-variable\">$PATH</span>:<span class=\"hljs-variable\">$HADOOP_INSTALL</span>/sbin\n<span class=\"hljs-built_in\">export</span> HADOOP_MAPRED_HOME=<span class=\"hljs-variable\">$HADOOP_INSTALL</span>\n<span class=\"hljs-built_in\">export</span> HADOOP_COMMON_HOME=<span class=\"hljs-variable\">$HADOOP_INSTALL</span>\n<span class=\"hljs-built_in\">export</span> HADOOP_HDFS_HOME=<span class=\"hljs-variable\">$HADOOP_INSTALL</span>\n<span class=\"hljs-built_in\">export</span> YARN_HOME=<span class=\"hljs-variable\">$HADOOP_INSTALL</span>\n</code></pre>\n<h5 id=\"usrlocalhadoopetchadoophadoop-envsh\">/usr/local/hadoop/etc/hadoop/hadoop-env.sh</h5>\n<p>还是跟上面一样,需要将 JDK 的路径设置成<code>JAVA_HOME</code></p>\n<pre><code>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/\n</code></pre>\n<h5 id=\"usrlocalhadoopetchadoopcore-sitexml\">/usr/local/hadoop/etc/hadoop/core-site.xml</h5>\n<p>在<code>&lt;configuartion&gt;&lt;/configuration&gt;</code>之间添加一个 fs.default.name,其值为 master 机器的 9000 端口. 譬如我的 master 机器是<code>elementary-os</code>,则 value 是<code>hdfs://elementary-os:9000</code> P.S.接下来的变量<code>{master-hostname}</code>请自行替换成自己的 master 的机器名.</p>\n<pre><code class=\"hljs xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">configuration</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>fs.default.name<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>hdfs://{master-hostname}:9000<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>hadoop.tmp.dir<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>file:/usr/local/hadoop_store/tmp<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">configuration</span>&gt;</span>\n</code></pre>\n<h4 id=\"usrlocalhadoopetchadoopyarn-sitexml\">/usr/local/hadoop/etc/hadoop/yarn-site.xml</h4>\n<p>在<code>&lt;configuartion&gt;&lt;/configuration&gt;</code>之间添加:</p>\n<pre><code class=\"hljs xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">configuration</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>yarn.resourcemanager.hostname<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>{master-hostname}<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">configuration</span>&gt;</span>\n\n</code></pre>\n<h5 id=\"usrlocalhadoopetchadoopmapred-sitexml\">/usr/local/hadoop/etc/hadoop/mapred-site.xml</h5>\n<p><code>mapred-site.xml</code>默认是不存在的. 但是有一份模板文件<code>mapred-site.xml.template</code>,我们将其复制并重命名成<code>mapred-site.xml</code></p>\n<pre><code class=\"hljs bash\">$ cp /usr/<span class=\"hljs-built_in\">local</span>/hadoop/etc/hadoop/mapred-site.xml.template /usr/<span class=\"hljs-built_in\">local</span>/hadoop/etc/hadoop/mapred-site.xml\n</code></pre>\n<p>在<code>&lt;configuartion&gt;&lt;/configuration&gt;</code>之间添加:</p>\n<pre><code class=\"hljs xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">configuartion</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>mapreduce.framework.name<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>yarn<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>mapred.job.tracker<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>{master-hostname}:9001<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>mapreduce.jobhistory.address<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>{master-hostname}:10020<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>{master-hostname}:19888<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">configuartion</span>&gt;</span>\n</code></pre>\n<h5 id=\"usrlocalhadoopetchadoophdfs-sitexml\">/usr/local/hadoop/etc/hadoop/hdfs-site.xml</h5>\n<p>在修改<code>hdfs-site.xml</code>这个配置文件之前,我们需要知道更多的一件事. hdfs 的块状文件,储存在一个指定的目录中. 按照官方文档的推荐,和网上一些文件夹的路径的约定,我们将这个 hdfs 的文件储存目录叫做<code>hadoop_store</code>.绝对路径为<code>/usr/local/hadoop_store</code></p>\n<p>于是 hadoop 的相关文件夹就变成了两个:</p>\n<blockquote>\n<p>/usr/local/hadoop /usr/local/hadoop_store</p>\n</blockquote>\n<p>由于读写权限问题,我们需要将<code>hadoop_store</code>的权限改成任意可读可写</p>\n<pre><code class=\"hljs bash\">$ sudo mkdir -p /usr/<span class=\"hljs-built_in\">local</span>/hadoop_store\n$ sudo chmod -R 777 /usr/<span class=\"hljs-built_in\">local</span>/hadoop_store\n</code></pre>\n<p>然后再在配置文件里面加入</p>\n<pre><code class=\"hljs xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">configuartion</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>{master-hostname}:50090<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>dfs.replication<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>1<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>dfs.namenode.name.dir<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>file:/usr/local/hadoop_store/hdfs/namenode<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">property</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">name</span>&gt;</span>dfs.datanode.data.dir<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">name</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">value</span>&gt;</span>file:/usr/local/hadoop_store/hdfs/datanode<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">value</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">property</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">configuartion</span>&gt;</span>\n</code></pre>\n<h5 id=\"slaves\">slaves</h5>\n<p><code>slaves</code>文件里面存储的是作为 slave 的节点的机器名. 以行为单位,一行一个. 默认只有一行 localhost. 从一般的集群角度来说,Master 不应该担当 Worker 的角色(老湿布置作业给小学僧,自己是不会一起做作业的) 所以 slaves 文件一般只写 slave 节点的名字,即 slave 节点作为 datanode,master 节点仅仅作为 namenode.</p>\n<p>但是由于我是一名好老湿,所以在本机配置中 master 也充当了 worker 的角色,所以本机是这样改的:</p>\n<pre><code>elementary-os\nhd-worker-a\nhd-worker-b\n</code></pre>\n<p>致此,所有的配置文件已经修改完毕. 可以将 master 上的 hadoop 文件夹压缩并且分发到各个 slave 节点上.</p>\n<h4 id=\"last-configure-format-namenode\">Last Configure : Format Namenode</h4>\n<p>最后一步配置,初始格式化 hdfs</p>\n<pre><code class=\"hljs bash\">$ <span class=\"hljs-built_in\">cd</span> /usr/<span class=\"hljs-built_in\">local</span>/hadoop/\n$ hdfs namenode -format\n</code></pre>\n<h3 id=\"start-all-hadoop-deamons\">Start all Hadoop deamons</h3>\n<p>启动 Hadoop 服务.</p>\n<pre><code class=\"hljs bash\">$ su hduser\n$ <span class=\"hljs-built_in\">cd</span> /usr/<span class=\"hljs-built_in\">local</span>/hadoop/\n$ sbin/start-dfs.sh\n$ sbin/start-yarn.sh\n</code></pre>\n<p>如果启动成功,在 master 节点上通过 jps 命令查看,应该包含如下 hadoop 进程</p>\n<pre><code>hduser@elementary-os:~$ jps\n51288 Jps\n22914 ResourceManager\n22361 NameNode\n23229 NodeManager\n22719 SecondaryNameNode\n</code></pre>\n<p>在 slave 节点上通过 jps 命令查看,应该包含如下 hadoop 进程</p>\n<pre><code>hduser@hd-worker-a:~$ jps\n6284 NodeManager\n6150 DateNode\n6409 Jps\n</code></pre>\n<p>或者可以通过浏览器访问<a href=\"https://master:8088\">https://master:8088</a> 或者<a href=\"https://master:50070\">https://master:50070</a> 查看 Hadoop 服务状态.</p>\n<p><img src=\"https://img.alicdn.com/tfscom/TB1vHd.MpXXXXXDXFXXXXXXXXXX.png\" alt=\"Nodes of the cluster\"> <img src=\"https://img.alicdn.com/tfscom/TB16f8YMpXXXXbJXVXXXXXXXXXX.png\" alt=\"Namenode information\"></p>\n<p>P.S.关于<code>jps</code>命令. jps 位于 jdk 的 bin 目录下,其作用是显示当前系统的 java 进程情况,及其 id 号. jps 相当于 linux 进程工具 ps,但是不支持管道命令 grep jps 并不使用应用程序名来查找 JVM 实例.</p>\n<h2 id=\"trouble-shooting\">Trouble Shooting</h2>\n<p>防跌坑指南. 记录了在 Hadoop 环境搭建过程中所遇到的坑</p>\n<h3 id=\"number-of-live-datanode0\">Number of Live DataNode:0</h3>\n<p>通过<code>start-dfs.sh</code>启动了 hadoop 多个节点的 datanode, 且通过<code>jps</code>命令能够看到正常的 datanode 和 resourcemanager 进程, 为什么 live datanode 数目为 0,或者只有 master 的那个 datanode?</p>\n<p>可通过以下方法排除:</p>\n<ol>\n<li>关闭所有节点的防火墙(ubuntu): 先查看防火墙状态</li>\n</ol>\n<pre><code class=\"hljs bash\">$ sudo ufw status\n</code></pre>\n<p>如果不是 disabled,则禁用</p>\n<pre><code class=\"hljs bash\">$ sudo ufw <span class=\"hljs-built_in\">disable</span>\n</code></pre>\n<ol start=\"2\">\n<li>在 hadoop 服务运行的时候,关闭 namenode 的安全模式</li>\n</ol>\n<pre><code class=\"hljs bash\">$ hadoop dfsadmin -safemode leave\n</code></pre>\n<ol start=\"3\">\n<li>在关闭 hadoop 服务的情况下,删除所有的日志文件,存储文件并重新 format 确保<code>hadoop_store</code>文件夹下的所有文件夹权限都是 777</li>\n</ol>\n<pre><code>$sudo rm -r /usr/local/hadoop/logs\n$sudo rm -r /usr/local/hadoop_store/tmp\n$sudo rm -r /usr/local/hadoop_store/hdfs\n$sidp hdfs namenode -format\n</code></pre>\n","metadata":{"title":"Hadoop 2.7.2 单节点与集群安装部署","created":"2016-04-09","updated":"2016-04-09","category":"Note","tags":["Hadoop","Linux"],"cover":"https://picsum.photos/id/278/800/300"},"summary":"最近需要做一些大数据相关项目,至少需要搭建 Hadoop 的基本环境. 由于用到的是目前版本号最高的 Hadoop 2.7.2.跟文档比较多的 2.6 以下的版本相比,在部署集群的时候遇见了很多坑. ","toc":[{"label":"Hadoop 2.7.2 单节点与集群安装部署","level":1,"id":"hadoop-272-单节点与集群安装部署","position":0,"children":[{"label":"Background","level":2,"id":"background","position":1,"children":[],"pid":0},{"label":"Table Of Content","level":2,"id":"table-of-content","position":2,"children":[],"pid":0},{"label":"Environment Setup","level":2,"id":"environment-setup","position":3,"children":[{"label":"Overview: How Developer deploy Hadoop in cluster mode","level":3,"id":"overview-how-developer-deploy-hadoop-in-cluster-mode","position":4,"children":[],"pid":3},{"label":"Network and Information about Virtual Machine","level":3,"id":"network-and-information-about-virtual-machine","position":5,"children":[],"pid":3},{"label":"Download Hadoop","level":3,"id":"download-hadoop","position":6,"children":[],"pid":3},{"label":"Add Hadoop Group and User","level":3,"id":"add-hadoop-group-and-user","position":7,"children":[],"pid":3},{"label":"Installing SSH and Copy Public Key to remote machine","level":3,"id":"installing-ssh-and-copy-public-key-to-remote-machine","position":8,"children":[],"pid":3},{"label":"Basic Environment Setup","level":3,"id":"basic-environment-setup","position":9,"children":[{"label":"JDK","level":4,"id":"jdk","position":10,"children":[],"pid":9}],"pid":3},{"label":"Configure Hadoop","level":3,"id":"configure-hadoop","position":11,"children":[{"label":"Unpack and move hadoop folder","level":4,"id":"unpack-and-move-hadoop-folder","position":12,"children":[],"pid":11},{"label":"Update Environment File","level":4,"id":"update-environment-file","position":13,"children":[{"label":"~/.bashrc","level":5,"id":"~bashrc","position":14,"children":[],"pid":13},{"label":"/usr/local/hadoop/etc/hadoop/hadoop-env.sh","level":5,"id":"usrlocalhadoopetchadoophadoop-envsh","position":15,"children":[],"pid":13},{"label":"/usr/local/hadoop/etc/hadoop/core-site.xml","level":5,"id":"usrlocalhadoopetchadoopcore-sitexml","position":16,"children":[],"pid":13}],"pid":11},{"label":"/usr/local/hadoop/etc/hadoop/yarn-site.xml","level":4,"id":"usrlocalhadoopetchadoopyarn-sitexml","position":17,"children":[{"label":"/usr/local/hadoop/etc/hadoop/mapred-site.xml","level":5,"id":"usrlocalhadoopetchadoopmapred-sitexml","position":18,"children":[],"pid":17},{"label":"/usr/local/hadoop/etc/hadoop/hdfs-site.xml","level":5,"id":"usrlocalhadoopetchadoophdfs-sitexml","position":19,"children":[],"pid":17},{"label":"slaves","level":5,"id":"slaves","position":20,"children":[],"pid":17}],"pid":11},{"label":"Last Configure : Format Namenode","level":4,"id":"last-configure-format-namenode","position":21,"children":[],"pid":11}],"pid":3},{"label":"Start all Hadoop deamons","level":3,"id":"start-all-hadoop-deamons","position":22,"children":[],"pid":3}],"pid":0},{"label":"Trouble Shooting","level":2,"id":"trouble-shooting","position":23,"children":[{"label":"Number of Live DataNode:0","level":3,"id":"number-of-live-datanode0","position":24,"children":[],"pid":23}],"pid":0}],"pid":-1}],"images":["https://img.alicdn.com/tfscom/TB1vHd.MpXXXXXDXFXXXXXXXXXX.png","https://img.alicdn.com/tfscom/TB16f8YMpXXXXbJXVXXXXXXXXXX.png"],"permalink":"/2016/04/09/hadoop-2-7-2-single-node-and-cluster-mode-installation-guide"}